{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer 구현하기_3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "1v8o4b19SHDGZWH2RIdsPZqNuff_kWsOK",
      "authorship_tag": "ABX9TyP+gB2+0qdKuwe0vUM1VrHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejvely/NLP_models/blob/main/Transformer_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 참조: https://paul-hyun.github.io/transformer-03/"
      ],
      "metadata": {
        "id": "omFSsOGKMjnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer 구현한기_2의 Transformer 클래스를 이용하여 NAver 영화리뷰 감정분석 분류 모델 클래스 정의"
      ],
      "metadata": {
        "id": "FkisadDZft4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Imports"
      ],
      "metadata": {
        "id": "Tqg3qLHDNXau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentencepiece import\n",
        "!pip install sentencepiece\n",
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNyswLZNM9dk",
        "outputId": "f83c2df0-bc0c-4a45-bccd-ff7c67244aa2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=f9236dfa6939a90773ec435c363cd66e3eaa80a0f4b389e21f37222e5b58c797\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm, tqdm_notebook, trange\n",
        "import sentencepiece as spm\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "TVWQtvVwNWrP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 설정\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3225qm3fcQ4",
        "outputId": "9a8be01a-cb92-4158-d229-c63f15d0f8e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 31 09:30:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Vocab\n",
        "\n",
        "sentencepiece를 활용해 만든 vocab을 이용해 텍스트를 입력 tensor로 변경"
      ],
      "metadata": {
        "id": "7pbW-Pk5Mymn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwEdp5QKMUZa",
        "outputId": "1ba625e0-a2ce-4b42-ce0e-2700cbb16049"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "# vocab loading\n",
        "vocab_file = '/content/drive/MyDrive/colab/NLP_모델/kowiki_corpus/kowiki.model'\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* SentencePieceProcessor class 참고\n",
        ": https://github.com/google/sentencepiece/blob/master/doc/api.md"
      ],
      "metadata": {
        "id": "-GKuRgqdOpqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(spm.SentencePieceProcessor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDuzjzwWOIS5",
        "outputId": "25f466af-6cab-43fa-f023-3456c93a48ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class SentencePieceProcessor in module sentencepiece:\n",
            "\n",
            "class SentencePieceProcessor(builtins.object)\n",
            " |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  Decode(self, input)\n",
            " |      Decode processed id or token sequences.\n",
            " |  \n",
            " |  DecodeIds = DecodeIdsWithCheck(self, ids)\n",
            " |  \n",
            " |  DecodeIdsAsSerializedProto = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
            " |  \n",
            " |  DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
            " |  \n",
            " |  DecodeIdsWithCheck(self, ids)\n",
            " |  \n",
            " |  DecodePieces(self, pieces)\n",
            " |  \n",
            " |  DecodePiecesAsSerializedProto(self, pieces)\n",
            " |  \n",
            " |  Detokenize = Decode(self, input)\n",
            " |  \n",
            " |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
            " |      Encode text input to segmented ids or tokens.\n",
            " |      \n",
            " |      Args:\n",
            " |      input: input string. accepsts list of string.\n",
            " |      out_type: output type. int or str.\n",
            " |      add_bos: Add <s> to the result (Default = false)\n",
            " |      add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
            " |        reversing (if enabled).\n",
            " |      reverse: Reverses the tokenized sequence (Default = false)\n",
            " |      nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
            " |                  nbest_size = {0,1}: No sampling is performed.\n",
            " |                  nbest_size > 1: samples from the nbest_size results.\n",
            " |                  nbest_size < 0: assuming that nbest_size is infinite and samples\n",
            " |                    from the all hypothesis (lattice) using\n",
            " |                    forward-filtering-and-backward-sampling algorithm.\n",
            " |      alpha: Soothing parameter for unigram sampling, and merge probability for\n",
            " |             BPE-dropout (probablity 'p' in BPE-dropout paper).\n",
            " |  \n",
            " |  EncodeAsIds(self, input)\n",
            " |  \n",
            " |  EncodeAsPieces(self, input)\n",
            " |  \n",
            " |  EncodeAsSerializedProto(self, input)\n",
            " |  \n",
            " |  GetEncoderVersion(self)\n",
            " |  \n",
            " |  GetPieceSize(self)\n",
            " |  \n",
            " |  GetScore = _batched_func(self, arg)\n",
            " |  \n",
            " |  IdToPiece = _batched_func(self, arg)\n",
            " |  \n",
            " |  Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
            " |      Initialzie sentencepieceProcessor.\n",
            " |      \n",
            " |      Args:\n",
            " |        model_file: The sentencepiece model file path.\n",
            " |        model_proto: The sentencepiece model serialized proto.\n",
            " |        out_type: output type. int or str.\n",
            " |        add_bos: Add <s> to the result (Default = false)\n",
            " |        add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
            " |          reversing (if enabled).\n",
            " |        reverse: Reverses the tokenized sequence (Default = false)\n",
            " |        nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
            " |                    nbest_size = {0,1}: No sampling is performed.\n",
            " |                    nbest_size > 1: samples from the nbest_size results.\n",
            " |                    nbest_size < 0: assuming that nbest_size is infinite and samples\n",
            " |                      from the all hypothesis (lattice) using\n",
            " |                      forward-filtering-and-backward-sampling algorithm.\n",
            " |        alpha: Soothing parameter for unigram sampling, and dropout probability of\n",
            " |          merge operations for BPE-dropout.\n",
            " |  \n",
            " |  IsByte = _batched_func(self, arg)\n",
            " |  \n",
            " |  IsControl = _batched_func(self, arg)\n",
            " |  \n",
            " |  IsUnknown = _batched_func(self, arg)\n",
            " |  \n",
            " |  IsUnused = _batched_func(self, arg)\n",
            " |  \n",
            " |  Load(self, model_file=None, model_proto=None)\n",
            " |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.\n",
            " |      \n",
            " |      Args:\n",
            " |        model_file: The sentencepiece model file path.\n",
            " |        model_proto: The sentencepiece model serialized proto. Either `model_file`\n",
            " |          or `model_proto` must be set.\n",
            " |  \n",
            " |  LoadFromFile(self, arg)\n",
            " |  \n",
            " |  LoadFromSerializedProto(self, serialized)\n",
            " |  \n",
            " |  LoadVocabulary(self, filename, threshold)\n",
            " |  \n",
            " |  NBestEncodeAsIds(self, input, nbest_size)\n",
            " |  \n",
            " |  NBestEncodeAsPieces(self, input, nbest_size)\n",
            " |  \n",
            " |  NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
            " |  \n",
            " |  PieceToId = _batched_func(self, arg)\n",
            " |  \n",
            " |  ResetVocabulary(self)\n",
            " |  \n",
            " |  SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  SetDecodeExtraOptions(self, extra_option)\n",
            " |  \n",
            " |  SetEncodeExtraOptions(self, extra_option)\n",
            " |  \n",
            " |  SetEncoderVersion(self, encoder_version)\n",
            " |  \n",
            " |  SetVocabulary(self, valid_vocab)\n",
            " |  \n",
            " |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
            " |  \n",
            " |  __getitem__(self, piece)\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __init__ = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
            " |  \n",
            " |  __len__(self)\n",
            " |  \n",
            " |  __repr__ = _swig_repr(self)\n",
            " |  \n",
            " |  __setstate__(self, serialized_model_proto)\n",
            " |  \n",
            " |  bos_id(self)\n",
            " |  \n",
            " |  decode = Decode(self, input)\n",
            " |  \n",
            " |  decode_ids = DecodeIdsWithCheck(self, ids)\n",
            " |  \n",
            " |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
            " |  \n",
            " |  decode_ids_as_serialized_proto_with_check = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
            " |  \n",
            " |  decode_ids_with_check = DecodeIdsWithCheck(self, ids)\n",
            " |  \n",
            " |  decode_pieces = DecodePieces(self, pieces)\n",
            " |  \n",
            " |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, pieces)\n",
            " |  \n",
            " |  detokenize = Decode(self, input)\n",
            " |  \n",
            " |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
            " |  \n",
            " |  encode_as_ids = EncodeAsIds(self, input)\n",
            " |  \n",
            " |  encode_as_pieces = EncodeAsPieces(self, input)\n",
            " |  \n",
            " |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input)\n",
            " |  \n",
            " |  eos_id(self)\n",
            " |  \n",
            " |  get_encoder_version = GetEncoderVersion(self)\n",
            " |  \n",
            " |  get_piece_size = GetPieceSize(self)\n",
            " |  \n",
            " |  get_score = _batched_func(self, arg)\n",
            " |  \n",
            " |  id_to_piece = _batched_func(self, arg)\n",
            " |  \n",
            " |  init = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
            " |  \n",
            " |  is_byte = _batched_func(self, arg)\n",
            " |  \n",
            " |  is_control = _batched_func(self, arg)\n",
            " |  \n",
            " |  is_unknown = _batched_func(self, arg)\n",
            " |  \n",
            " |  is_unused = _batched_func(self, arg)\n",
            " |  \n",
            " |  load = Load(self, model_file=None, model_proto=None)\n",
            " |  \n",
            " |  load_from_file = LoadFromFile(self, arg)\n",
            " |  \n",
            " |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)\n",
            " |  \n",
            " |  load_vocabulary = LoadVocabulary(self, filename, threshold)\n",
            " |  \n",
            " |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size)\n",
            " |  \n",
            " |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size)\n",
            " |  \n",
            " |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
            " |  \n",
            " |  pad_id(self)\n",
            " |  \n",
            " |  piece_size(self)\n",
            " |  \n",
            " |  piece_to_id = _batched_func(self, arg)\n",
            " |  \n",
            " |  reset_vocabulary = ResetVocabulary(self)\n",
            " |  \n",
            " |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
            " |  \n",
            " |  serialized_model_proto(self)\n",
            " |  \n",
            " |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)\n",
            " |  \n",
            " |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)\n",
            " |  \n",
            " |  set_encoder_version = SetEncoderVersion(self, encoder_version)\n",
            " |  \n",
            " |  set_vocabulary = SetVocabulary(self, valid_vocab)\n",
            " |  \n",
            " |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
            " |  \n",
            " |  unk_id(self)\n",
            " |  \n",
            " |  vocab_size(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  thisown\n",
            " |      The membership flag\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Config\n",
        "\n",
        "설정을 json 형태로 저장하고, 이를 읽어서 처리하는 간단한 클래스"
      ],
      "metadata": {
        "id": "F6Aa0FwMMz9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"configuration json을 읽어들이는 class\"\"\"\n",
        "class Config(dict):\n",
        "  __getattr__ = dict.__getitem__\n",
        "  __setarrt__ = dict.__setitem__\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, file):\n",
        "    with open(file, 'r') as f:\n",
        "      config = json.loads(f.read())\n",
        "      return Config(config)"
      ],
      "metadata": {
        "id": "15KSw-8fM79x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config({\n",
        "    'n_enc_vocab':len(vocab),\n",
        "    'n_dec_vocab':len(vocab),\n",
        "    'n_enc_seq':256,\n",
        "    'n_dec_seq':256,\n",
        "    'n_layer':6,\n",
        "    'd_hidn':256,\n",
        "    'i_pad':0,\n",
        "    'd_ff':1024,\n",
        "    'n_head':4,\n",
        "    'd_head':64,\n",
        "    'dropout':0.1,\n",
        "    'layer_norm_epsilon':1e-12\n",
        "})\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSFpfW08N8Hz",
        "outputId": "db671edb-a58d-4e4e-92c2-38e69ccbf4d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Common Class\n",
        "\n",
        "Transformer 구현하기_1.ipynb 에서 구현한 'Position Embedding','Multi-Head Attention','Feed Forward'의 코드"
      ],
      "metadata": {
        "id": "Jzs2ENBtOaVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Position Encoding\n",
        "\n",
        "Position Embedding의 초기 값을 구하는 함수\n",
        "1. 각 position별 hidden index별 angle 값을 구합니다.\n",
        "2. hidden 짝수 index의 angle값의 sin값을 구합니다.\n",
        "3. hidden 홀수 index의 angle값의 cos값을 구합니다."
      ],
      "metadata": {
        "id": "2b7EceoKP62L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"sinusoid position encoding\"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "  def cal_angle(position, i_hidn):\n",
        "    return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
        "  def get_posi_angle_vec(position):\n",
        "    return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "\n",
        "  sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])  # 1. 각 position별 hidden index별 angle 값을 구합니다.\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])                         # 2. hidden 짝수 index의 angle값의 sin값을 구합니다.\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])                         # 3. hidden 홀수 index의 angle값의 cos값을 구합니다.\n",
        "\n",
        "  return sinusoid_table"
      ],
      "metadata": {
        "id": "6n6xR_-dOYV1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Pad Mask\n",
        "\n",
        "Attention을 구할 때 Padding 부분을 제외하기 위한 Mask를 구하는 함수\n",
        "\n",
        "1. K의 값 중에 Pad인 부분을 True로 변경합니다.(나머지는 False)\n",
        "2. 구해진 값의 크기를 Q-len, K-len 되도록 변경합니다."
      ],
      "metadata": {
        "id": "ey2_NMszQEm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"attention pad mask\"\"\"\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
        "  batch_size, len_q = seq_q.size()\n",
        "  batch_size, len_k = seq_k.size()\n",
        "  pad_attn_mask = seq_k.data.eq(i_pad)                                              # 1. K의 값 중에 Pad인 부분을 True로 변경합니다.(나머지는 False)\n",
        "  pad_attn_mask = pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)       # 2. 구해진 값의 크기를 Q-len, K-len 되도록 변경합니다.\n",
        "  return pad_attn_mask"
      ],
      "metadata": {
        "id": "Ozn9JwhOPHIG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Decoder Mask\n",
        "\n",
        "Decoder의 'Masked Multi Head Attention'에서 사용할 Mask를 구하는 함수입니다.\n",
        "\n",
        "현재 단어와 이전 단어는 볼 수 있고 다음 단어는 볼 수 없도록 Masking 합니다.\n",
        "\n",
        "1. 모든 값이 1인 Q-len, K-leb 테이블을 생성합니다.\n",
        "2. 대각선을 기준으로 아래쪽을 0으로 만듭니다."
      ],
      "metadata": {
        "id": "2tL99-xMQ7P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"attention decoder mask\"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "  subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1)) # 1. 모든 값이 1인 Q-len, K-leb 테이블을 생성합니다.\n",
        "  subsequent_mask = subsequent_mask.triu(diagonal=1)                                                 # 2. 대각선을 기준으로 아래쪽을 0으로 만듭니다.\n",
        "  return subsequent_mask"
      ],
      "metadata": {
        "id": "g9LhHwhmQ4fw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled Dot Product Attention\n",
        "\n",
        "Scaled Dot Product Attention을 구하는 클래스\n",
        "\n",
        "1. Q * K.transpose를 구합니다.\n",
        "2. K-dimention에 루트를 취한 값으로 나눠 줍니다.\n",
        "3. Mask를 적용합니다.\n",
        "4. Softmax를 취해 각 단어의 가중치 확률분포 attn_prob를 구합니다.\n",
        "5. attn_prob * V를 구합니다. 구한 값은 Q에 대한 V의 가중치 합 벡터입니다."
      ],
      "metadata": {
        "id": "PMpcf2hGRowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"scale dot product attention\"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.dropout= nn.Dropout(config.dropout)\n",
        "    self.scale = 1 / (self.config.d_head ** 0.5)\n",
        "\n",
        "  def forward(self, Q, K, V, attn_mask):\n",
        "    scores = torch.matmul(Q, K.transpose(-1, -2))     # 1. Q * K.transpose를 구합니다.\n",
        "    scores = scores.mul_(self.scale)                   # 2. K-dimention에 루트를 취한 값으로 나눠 줍니다.\n",
        "    scores.masked_fill_(attn_mask, -1e9)              # 3. Mask를 적용합니다.\n",
        "\n",
        "    attn_prob = nn.Softmax(dim= -1)(scores)           # 4. Softmax를 취해 각 단어의 가중치 확률분포 attn_prob를 구합니다.\n",
        "    attn_prob = self.dropout(attn_prob)               # 5. attn_prob * V를 구합니다. 구한 값은 Q에 대한 V의 가중치 합 벡터입니다.\n",
        "\n",
        "    context = torch.matmul(attn_prob, V)\n",
        "\n",
        "    return context, attn_prob"
      ],
      "metadata": {
        "id": "WdOQ09aBRmnS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention을 구하는 클래스\n",
        "\n",
        "1. Q * W_Q를 한 후 multi-head로 나눕니다.\n",
        "2. K * W_K를 한 후 multi-head로 나눕니다.\n",
        "3. V * W_V를 한 후 multi-head로 나눕니다.\n",
        "4. ScaledDotProductAttention 클래스를 이용해 각 head 별 Attention을 구합니다.\n",
        "5. 여러 개의 head를 1개로 합칩니다.\n",
        "6. Linear를 취해 최종 Multi-Head Attention값을 구합니다."
      ],
      "metadata": {
        "id": "RklKvjm9TAD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"multi head attention\"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "    self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "    self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "    self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
        "    self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, Q, K, V, attn_mask):\n",
        "    batch_size = Q.size(0)\n",
        "    q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)                   # 1. Q * W_Q를 한 후 multi-head로 나눕니다.\n",
        "    k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)                   # 2. K * W_K를 한 후 multi-head로 나눕니다.\n",
        "    v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)                   # 3. V * W_V를 한 후 multi-head로 나눕니다.\n",
        "\n",
        "    attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "\n",
        "    context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)                                             # 4. ScaledDotProductAttention 클래스를 이용해 각 head 별 Attention을 구합니다.\n",
        "    context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)    # 5. 여러 개의 head를 1개로 합칩니다.\n",
        "    output = self.linear(context)                                                                                   # 6. Linear를 취해 최종 Multi-Head Attention값을 구합니다.\n",
        "    output = self.dropout(output)\n",
        "    \n",
        "    return output, attn_prob\n"
      ],
      "metadata": {
        "id": "jcfgPFY8S-0u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feed Forward\n",
        "\n",
        "FeedForward를 처리하는 클래스입니다.\n",
        "\n",
        "1. Linear를 실행하여 shape을 d_ff(hidden * 4)크기로 키웁니다.\n",
        "2. activation 함수(relu or gelu)를 실행합니다.\n",
        "3. Linear를 실행하여 shape을 hidden 크기로 줄입니다."
      ],
      "metadata": {
        "id": "dj9aEP3HVyZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"feed forward\"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels = self.config.d_hidn, out_channels = self.config.d_ff, kernel_size=1)\n",
        "    self.conv2 = nn.Conv1d(in_channels = self.config.d_ff, out_channels = self.config.d_hidn, kernel_size=1)\n",
        "    self.active = F.gelu\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    output = self.conv1(inputs.transpose(1, 2))     # 1. Linear를 실행하여 shape을 d_ff(hidden * 4)크기로 키웁니다.\n",
        "    output = self.active(output)                    # 2. activation 함수(relu or gelu)를 실행합니다.\n",
        "\n",
        "    output = self.conv2(output).transpose(1, 2)     # 3. Linear를 실행하여 shape을 hidden 크기로 줄입니다.\n",
        "    output = self.dropout(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "zT527TcmVw6q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Encoder"
      ],
      "metadata": {
        "id": "IOw--0ERbdiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder layer\n",
        "\n",
        "Encoder에서 루프를 돌며 처리할 수 있도록 EncoderLayer를 정의하고 여러 개 만들어서 실행\n",
        "\n",
        "1. Multi-Head Attention을 수행합니다. Q, K, V 모두 동일한 값을 사용하는 self-attention입니다.\n",
        "2. 1번의 결과와 input(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "3. 2번의 결과를 입력으로 Feed Forward를 실행합니다.\n",
        "4. 3번의 결과와 2번의 결과(residual)을 더한 후 LayerNorm을 실행합니다."
      ],
      "metadata": {
        "id": "tWuxESpiXq3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"encoder layer\"\"\"\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(self.config)\n",
        "    self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "    self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "    self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "\n",
        "  def forward(self, inputs, attn_mask):\n",
        "    attn_outputs, attn_prob = self.self_attn(inputs, inputs, inputs,attn_mask)    # 1. Multi-Head Attention을 수행합니다. Q, K, V 모두 동일한 값을 사용하는 self-attention입니다.\n",
        "    attn_outputs = self.layer_norm1(inputs + attn_outputs)                        # 2. 1번의 결과와 input(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "\n",
        "    ffn_outputs = self.pos_ffn(attn_outputs)                                      # 3. 2번의 결과를 입력으로 Feed Forward를 실행합니다.\n",
        "    ffn_outputs = self.layer_norm2(ffn_outputs + attn_outputs)                    # 4. 3번의 결과와 2번의 결과(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "\n",
        "    return ffn_outputs, attn_prob"
      ],
      "metadata": {
        "id": "P2fcz_tXXaIZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "Encoder 클래스\n",
        "\n",
        "1. 입력에 대한 Position 값을 구합니다.\n",
        "2. Input Embedding과 Position Embedding을 구한 후 더합니다.\n",
        "3. 입력에 대한 attention pad mask를 구합니다.\n",
        "4. for 루프를 돌며 각 layer를 실행합니다. layer의 입력은 이전 layer의 출력 값입니다."
      ],
      "metadata": {
        "id": "a3ytSH9aZYDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"encoder\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
        "    sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\n",
        "    self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # 1. 입력에 대한 Position 값을 구합니다.\n",
        "    positions = torch.arange(inputs.size(1), device = inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
        "    pos_mask = inputs.eq(self.config.i_pad)\n",
        "    positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "    outputs = self.enc_emb(inputs) + self.pos_emb(positions)          # 2. Input Embedding과 Position Embedding을 구한 후 더합니다.\n",
        "\n",
        "    attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)  # 3. 입력에 대한 attention pad mask를 구합니다.\n",
        "\n",
        "    attn_probs = []\n",
        "    for layer in self.layers:\n",
        "      outputs, attn_prob = layer(outputs, attn_mask)                  # 4. for 루프를 돌며 각 layer를 실행합니다. layer의 입력은 이전 layer의 출력 값입니다.\n",
        "      attn_probs.append(attn_prob)\n",
        "\n",
        "    return outputs, attn_probs"
      ],
      "metadata": {
        "id": "XDkjdsC9ZWoL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Decoder"
      ],
      "metadata": {
        "id": "arYYs5DGbV85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder Layer\n",
        "\n",
        "Decoder에서 루프를 돌며 처리할 수 있도록 DecoderLayer를 정의하고 여러 개 만들어서 실행\n",
        "\n",
        "1. Multi-Head Attention을 수행합니다.\n",
        "- Q, K, V 모두 동일한 값을 사용하는 self-attention입니다.\n",
        "2. 1번의 결과와 input(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "3. Encoder-Decoder Multi-Head Attention을 수행합니다.\n",
        "- Q: 2번의 결과\n",
        "- K, V: Encoder 결과\n",
        "4. 3번의 결과와 2번의 결과(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "5. 4번의 결과를 입력으로 FeedForward를 실행합니다.\n",
        "6. 5번의 결과와 4번의 결과(residual)을 더한 후 LayerNorm을 실행합니다."
      ],
      "metadata": {
        "id": "8ExJrN8YbfDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"decoder layer\"\"\"\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(self.config)\n",
        "    self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "    self.dec_enc_attn = MultiHeadAttention(self.config)\n",
        "    self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "    self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "    self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "\n",
        "  def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
        "    self_attn_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)      # 1. Multi-Head Attention을 수행합니다. - Q, K, V 모두 동일한 값을 사용하는 self-attention입니다.\n",
        "    self_attn_outputs = self.layer_norm1(dec_inputs + self_attn_outputs)                                        # 2. 1번의 결과와 input(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "\n",
        "    # 3. Encoder-Decoder Multi-Head Attention을 수행합니다.\n",
        "    # - Q: 2번의 결과\n",
        "    # - K, V: Encoder 결과\n",
        "    dec_enc_attn_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_attn_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "    dec_enc_attn_outputs = self.layer_norm2(self_attn_outputs + dec_enc_attn_outputs)                           # 4. 3번의 결과와 2번의 결과(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "\n",
        "    ffn_outputs = self.pos_ffn(dec_enc_attn_outputs)                                                            # 5. 4번의 결과를 입력으로 FeedForward를 실행합니다.\n",
        "    ffn_outputs = self.layer_norm3(dec_enc_attn_outputs + ffn_outputs)                                          # 6. 5번의 결과와 4번의 결과(residual)을 더한 후 LayerNorm을 실행합니다.\n",
        "    \n",
        "    return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
      ],
      "metadata": {
        "id": "ptYhWHItbRaI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n",
        "Decoder 클래스\n",
        "\n",
        "1. 입력에 대한 Position 값을 구합니다.\n",
        "2. Input Embedding과 Position Embedding을 구한 후 더합니다.\n",
        "3. 입력에 대한 attention pad mask를 구합니다.\n",
        "4. 입력에 대한 decoder attention mask를 구합니다.\n",
        "5. attention pad mask와 decoder attention mask 중 1곳이라도 mask 되어 있는 부분이 mask 되도록 attention mask를 구합니다.\n",
        "6. Q(decoder input), K(encoder output)에 대한 attention mask를 구합니다.\n",
        "7. for 루프를 돌며 각 layer를 실행합니다.\n",
        "- layer의 입력은 이전 layer의 출력 값입니다."
      ],
      "metadata": {
        "id": "ySTLJ0FJeUSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"decoder\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
        "    sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
        "    self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze = True)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "\n",
        "  def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
        "    # 1. 입력에 대한 Position 값을 구합니다.\n",
        "    positions = torch.arange(dec_inputs.size(1), device = dec_inputs.device, dtype = dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous()+ 1\n",
        "    pos_mask = dec_inputs.eq(self.config.i_pad)\n",
        "    positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "    dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)                    # 2. Input Embedding과 Position Embedding을 구한 후 더합니다.\n",
        "\n",
        "    dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)    # 3. 입력에 대한 attention pad mask를 구합니다.\n",
        "    dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)                           # 4. 입력에 대한 decoder attention mask를 구합니다.\n",
        "    dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)       # 5. attention pad mask와 decoder attention mask 중 1곳이라도 mask 되어 있는 부분이 mask 되도록 attention mask를 구합니다.\n",
        "    dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)    # 6. Q(decoder input), K(encoder output)에 대한 attention mask를 구합니다.\n",
        "\n",
        "    self_attn_probs, dec_enc_attn_probs = [], []\n",
        "    # 7. for 루프를 돌며 각 layer를 실행합니다.\n",
        "    # - layer의 입력은 이전 layer의 출력 값입니다.\n",
        "    for layer in self.layers:\n",
        "      dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "      self_attn_probs.append(self_attn_prob)\n",
        "      dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
        "    return dec_outputs, self_attn_probs, dec_enc_attn_probs"
      ],
      "metadata": {
        "id": "l9m3im8MeSF-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Transformer\n",
        "\n",
        "Transformer 클래스\n",
        "\n",
        "1. Encoder Input을 입력으로 Encoder를 실행합니다.\n",
        "2. Encoder Output과 Decoder Input을 입력으로 Decoder를 실행합니다."
      ],
      "metadata": {
        "id": "5Pw-lqNihZy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"transformer\"\"\"\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.encoder = Encoder(self.config)\n",
        "    self.decoder = Decoder(self.config)\n",
        "\n",
        "  def forward(self, enc_inputs, dec_inputs):\n",
        "    enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)                                                 # 1. Encoder Input을 입력으로 Encoder를 실행합니다.\n",
        "    dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)    # 2. Encoder Output과 Decoder Input을 입력으로 Decoder를 실행합니다.\n",
        "    return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
      ],
      "metadata": {
        "id": "wQGaSlHbhV58"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model\n",
        "\n",
        "Transforemr 클래스를 이용하여 Naver 영화리뷰 감정분석 분류 모델 클래스를 정의\n",
        "\n",
        "1. Encoder input과 Decoder inurt을 입력으로 Transforemr 모델을 실행합니다.\n",
        "2. Transformer 출력의 max 값을 구합니다.\n",
        "3. Linear를 실행하여 최종 예측 결과를 만듭니다."
      ],
      "metadata": {
        "id": "AiGfAxSif2Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"naver movie classifiaction\"\"\"\n",
        "class MovieClassification(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = Transformer(self.config)\n",
        "    self.projection = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
        "\n",
        "  def forward(self, enc_inputs, dec_inputs):\n",
        "    dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)  # 1. Encoder input과 Decoder inurt을 입력으로 Transforemr 모델을 실행합니다.\n",
        "    dec_outputs, _ = torch.max(dec_outputs, dim=1)                                                                        # 2. Transformer 출력의 max 값을 구합니다.\n",
        "    logits = self.projection(dec_outputs)                                                                                  # 3. Linear를 실행하여 최종 예측 결과를 만듭니다.\n",
        "\n",
        "    return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
      ],
      "metadata": {
        "id": "TUe0yS7df1z5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset"
      ],
      "metadata": {
        "id": "6_YZIdRJhehO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "Naber 영화리뷰 감정분석 데이터 셋\n",
        "\n",
        "1. 입력 파일로부터 'label'을 읽어 들입니다.\n",
        "2. 입력 파일로부터 'doc'token을 읽어 숫자(token id)로 변경합니다.\n",
        "3. Decoder 입력은 '[BOS]'로 고정합니다."
      ],
      "metadata": {
        "id": "q1Bh7TCNhh_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"영화 분류 데이터셋\"\"\"\n",
        "class MovieDataSet(torch.utils.data.Dataset):\n",
        "  def __init__(self, vocab, infile):\n",
        "    self.vocab = vocab\n",
        "    self.labels = []\n",
        "    self.sentences = []\n",
        "\n",
        "    line_cnt = 0\n",
        "    with open(infile, 'r') as f:\n",
        "      for line in f:\n",
        "        line_cnt += 1\n",
        "    \n",
        "    with open(infile, 'r') as f:\n",
        "      for i, line in enumerate(tqdm(f, total=line_cnt, desc=f'Loading {infile}', unit=' lines')):\n",
        "        data = json.loads(line)\n",
        "        self.labels.append(data['label'])                                           # 1. 입력 파일로부터 'label'을 읽어 들입니다.\n",
        "        self.sentences.append([vocab.piece_to_id(p) for p in data['doc']])          # 2. 입력 파일로부터 'doc'token을 읽어 숫자(token id)로 변경합니다.\n",
        "\n",
        "  def __len__(self):\n",
        "    assert len(self.labels) == len(self.sentences)\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return (torch.tensor(self.labels[item]),\n",
        "            torch.tensor(self.sentences[item]),\n",
        "            torch.tensor([self.vocab.piece_to_id('[BOS]')]))                        # 3. Decoder 입력은 '[BOS]'로 고정합니다."
      ],
      "metadata": {
        "id": "VIwWx-qwhbq_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collate_fn\n",
        "\n",
        "배치단위로 데이터 처리를 위한 collate_fn입니다.\n",
        "\n",
        "1. Encoder inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가합니다.\n",
        "- padding은 sentencepiece를 활용해 vocab 만들기에서 '-pad_id=0'옵셥으로 지정한 값입니다.\n",
        "2. Decoder inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가합니다.\n",
        "3. Label은 길이가 1 고정이므로 stack 함수를 이용해 tensor로 만듭니다."
      ],
      "metadata": {
        "id": "oZYfOKFGjNLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" movie data collate_fn \"\"\"\n",
        "def movie_collate_fn(inputs):\n",
        "  labels, enc_inputs, dec_inputs = list(zip(*inputs))\n",
        "\n",
        "  enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first = True, padding_value=0)     # 1. Encoder inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가합니다.\n",
        "  dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first = True, padding_value=0)     # 2. Decoder inputs의 길이가 같아지도록 짧은 문장에 padding(0)을 추가합니다.\n",
        "\n",
        "  batch = [\n",
        "           torch.stack(labels, dim = 0),                                                            # 3. Label은 길이가 1 고정이므로 stack 함수를 이용해 tensor로 만듭니다.\n",
        "           enc_inputs,\n",
        "           dec_inputs\n",
        "  ]\n",
        "  return batch"
      ],
      "metadata": {
        "id": "Dj2G0VOVjKCh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DataLoader\n",
        "\n",
        "위에서 정의한 Dataset과 collate_fn을 이용해 학습용 (train_loader), 평가용(test_loader) Data Loader를 만듭니다."
      ],
      "metadata": {
        "id": "OpZMHaVQkei-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_dataset = MovieDataSet(vocab, '/content/drive/MyDrive/colab/NLP_모델/naver_sentiment_corpus/ratings_train.json')\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True, collate_fn = movie_collate_fn)\n",
        "test_dataset = MovieDataSet(vocab, '/content/drive/MyDrive/colab/NLP_모델/naver_sentiment_corpus/ratings_test.json')\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn = movie_collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eshTcx58kd6i",
        "outputId": "68c6b9b1-231e-4930-dbc8-928c13e23bfc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading /content/drive/MyDrive/colab/NLP_모델/naver_sentiment_corpus/ratings_train.json: 100%|██████████| 149995/149995 [00:04<00:00, 32190.74 lines/s]\n",
            "Loading /content/drive/MyDrive/colab/NLP_모델/naver_sentiment_corpus/ratings_test.json: 100%|██████████| 49997/49997 [00:01<00:00, 30192.94 lines/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluate\n",
        "\n",
        "학습된 MovieClassification 모델의 성능을 평가하기 위한 함수입니다. 평가는 정확도(accuracy)를 사용했습니다.\n",
        "\n",
        "1. Encoder input과 Decoder input을 입력으로 MovieClassification을 실행합니다.\n",
        "2. 1번의 결과 중 첫번째 값이 예측 logits입니다.\n",
        "3. lgits의 최대값의 index를 구합니다.\n",
        "4. 3번에게 구한 값과 labels의 값이 같은지 비교합니다."
      ],
      "metadata": {
        "id": "dn9eSfJVlTNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"모델 epoch 평가\"\"\"\n",
        "def eval_epoch(config, model, data_loader):\n",
        "  matchs = []\n",
        "  model.eval()\n",
        "\n",
        "  n_word_total = 0\n",
        "  n_correct_total = 0\n",
        "  with tqdm(total=len(data_loader), desc = f'Valid') as pbar:\n",
        "    for i, value in enumerate(data_loader):\n",
        "      labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "      outputs = model(enc_inputs, dec_inputs)       # 1. Encoder input과 Decoder input을 입력으로 MovieClassification을 실행합니다.\n",
        "      logits = outputs[0]                           # 2. 1번의 결과 중 첫번째 값이 예측 logits입니다.\n",
        "      _, indices = logits.max(1)                     # 3. lgits의 최대값의 index를 구합니다.\n",
        "\n",
        "      match = torch.eq(indices, labels).detach()    # 4. 3번에게 구한 값과 labels의 값이 같은지 비교합니다.\n",
        "      matchs.extend(match.cpu())\n",
        "      accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
        "\n",
        "      pbar.update(1)\n",
        "      pbar.set_postfix_str(f'Acc: {accuracy:.3f}')\n",
        "\n",
        "  return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
      ],
      "metadata": {
        "id": "zNSRKmgElRRu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train\n",
        "\n",
        "MovieClassification 모델을 학습하기 위한 함수입니다.\n",
        "\n",
        "1. Encoder input과 Decoder input을 입력으로 MovieClassification을 실행합니다.\n",
        "2. 1번의 결과 중 첫 번째 값이 예측 logits입니다.\n",
        "3. logits 값과 labels의 값을 이용해 Loss를 계산합니다.\n",
        "4. loss, optimizer를 이용해 학습합니다."
      ],
      "metadata": {
        "id": "iY4ZKIuanUYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"모델 epoch 학습\"\"\"\n",
        "def train_epoch(config, epoch, model, criterion, optimizer, train_loaer):\n",
        "  losses = []\n",
        "  model.train()\n",
        "\n",
        "  with tqdm(total=len(train_loader), desc = f'Train {epoch}') as pbar:\n",
        "    for i, value in enumerate(train_loader):\n",
        "      labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(enc_inputs, dec_inputs)                    # 1. Encoder input과 Decoder input을 입력으로 MovieClassification을 실행합니다.\n",
        "      logits = outputs[0]                                        # 2. 1번의 결과 중 첫 번째 값이 예측 logits입니다.\n",
        "\n",
        "      loss = criterion(logits, labels)                           # 3. logits 값과 labels의 값을 이용해 Loss를 계산합니다.\n",
        "      loss_val = loss.item()\n",
        "      losses.append(loss_val)\n",
        "\n",
        "      loss.backward()                                            # 4. loss, optimizer를 이용해 학습합니다.\n",
        "      optimizer.step()\n",
        "\n",
        "      pbar.update()\n",
        "      pbar.set_postfix_str(f'Loss: {loss_val:.3f} ({np.mean(losses):.3f}')\n",
        "  return np.mean(losses)"
      ],
      "metadata": {
        "id": "J9ZRdNRDnS6I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습을 위한 추가적인 내용을 선언합니다.\n",
        "\n",
        "1. GPU 사용 여부를 확인합니다.\n",
        "2. 출력 값 개수를 정의합니다.(부정(0), 긍정(1) 2가지입니다.)\n",
        "3. learning_rate 및 학습 epoch를 선언합니다."
      ],
      "metadata": {
        "id": "hJQwb1xwqL06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # 1. GPU 사용 여부를 확인합니다.\n",
        "config.n_output = 2                                                             # 2. 출력 값 개수를 정의합니다.(부정(0), 긍정(1) 2가지입니다.)\n",
        "print(config)\n",
        "\n",
        "learning_rate = 5e-5                                                            # 3. learning_rate 및 학습 epoch를 선언합니다.\n",
        "n_epoch = 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jruGPGaqK0U",
        "outputId": "15ebb56e-3b0e-486c-8a94-85da774016bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 선언된 내용을 이용해 학습을 실행하는 절차입니다.\n",
        "\n",
        "1. MovieClassification을 생성합니다.\n",
        "2. MovieClassification이 GPU 또는 CPU를 지원하도록 합니다.\n",
        "3. loss 함수를 선언합니다.\n",
        "4. optimizer를 선언합니다.\n",
        "5. 각 epoch 마다 학습을 합니다.\n",
        "6. 각 epoch 마다 평가를 합니다."
      ],
      "metadata": {
        "id": "Rm97eXCHrIqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 약 45분 소요\n",
        "model = MovieClassification(config)                                                   # 1. MovieClassification을 생성합니다.\n",
        "model.to(config.device)                                                               # 2. MovieClassification이 GPU 또는 CPU를 지원하도록 합니다.\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()                                               # 3. loss 함수를 선언합니다.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)                    # 4. optimizer를 선언합니다.\n",
        "\n",
        "losses, scores = [], []\n",
        "for epoch in range(n_epoch):\n",
        "  loss = train_epoch(config, epoch, model, criterion, optimizer, train_loader)        # 5. 각 epoch 마다 학습을 합니다.\n",
        "  score = eval_epoch(config, model, test_loader)                                      # 6. 각 epoch 마다 평가를 합니다.\n",
        "\n",
        "  losses.append(loss)\n",
        "  scores.append(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZwhxLf7rEZg",
        "outputId": "6bc467e3-e2db-4a09-9631-ed3fde3f2771"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 0: 100%|██████████| 1172/1172 [03:40<00:00,  5.32it/s, Loss: 0.312 (0.484]\n",
            "Valid: 100%|██████████| 391/391 [00:45<00:00,  8.51it/s, Acc: 0.805]\n",
            "Train 1: 100%|██████████| 1172/1172 [03:39<00:00,  5.34it/s, Loss: 0.404 (0.402]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s, Acc: 0.817]\n",
            "Train 2: 100%|██████████| 1172/1172 [03:38<00:00,  5.35it/s, Loss: 0.338 (0.375]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.26it/s, Acc: 0.815]\n",
            "Train 3: 100%|██████████| 1172/1172 [03:38<00:00,  5.36it/s, Loss: 0.354 (0.355]\n",
            "Valid: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s, Acc: 0.829]\n",
            "Train 4: 100%|██████████| 1172/1172 [03:40<00:00,  5.32it/s, Loss: 0.315 (0.335]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.30it/s, Acc: 0.826]\n",
            "Train 5: 100%|██████████| 1172/1172 [03:40<00:00,  5.32it/s, Loss: 0.301 (0.315]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s, Acc: 0.832]\n",
            "Train 6: 100%|██████████| 1172/1172 [03:40<00:00,  5.32it/s, Loss: 0.211 (0.294]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.27it/s, Acc: 0.833]\n",
            "Train 7: 100%|██████████| 1172/1172 [03:39<00:00,  5.35it/s, Loss: 0.422 (0.271]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s, Acc: 0.831]\n",
            "Train 8: 100%|██████████| 1172/1172 [03:39<00:00,  5.33it/s, Loss: 0.372 (0.249]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.26it/s, Acc: 0.832]\n",
            "Train 9: 100%|██████████| 1172/1172 [03:39<00:00,  5.35it/s, Loss: 0.302 (0.227]\n",
            "Valid: 100%|██████████| 391/391 [00:42<00:00,  9.26it/s, Acc: 0.835]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Result"
      ],
      "metadata": {
        "id": "AtNAYkd51B0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# table\n",
        "data = {\n",
        "    'loss': losses,\n",
        "    'score': scores\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12,4])\n",
        "plt.plot(losses, label = 'loss')\n",
        "plt.plot(scores, label = 'score')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "lAMHRErD1GjC",
        "outputId": "db3ff302-ec68-4992-ea75-2e3bec2b5522"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cf51eec1-9628-428c-9aa2-86ae5fb149fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.483909</td>\n",
              "      <td>0.804508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.401940</td>\n",
              "      <td>0.817049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.375420</td>\n",
              "      <td>0.815269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.354822</td>\n",
              "      <td>0.828550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.335374</td>\n",
              "      <td>0.826430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.315077</td>\n",
              "      <td>0.831870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.294090</td>\n",
              "      <td>0.833450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.271483</td>\n",
              "      <td>0.830770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.248893</td>\n",
              "      <td>0.831590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.226699</td>\n",
              "      <td>0.834590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf51eec1-9628-428c-9aa2-86ae5fb149fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf51eec1-9628-428c-9aa2-86ae5fb149fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf51eec1-9628-428c-9aa2-86ae5fb149fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       loss     score\n",
              "0  0.483909  0.804508\n",
              "1  0.401940  0.817049\n",
              "2  0.375420  0.815269\n",
              "3  0.354822  0.828550\n",
              "4  0.335374  0.826430\n",
              "5  0.315077  0.831870\n",
              "6  0.294090  0.833450\n",
              "7  0.271483  0.830770\n",
              "8  0.248893  0.831590\n",
              "9  0.226699  0.834590"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAEGCAYAAABM2KIzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXScd33v8c93du3yIsmLrNgpTiCQBAfZFLiEkoWkQBMoS2yaW0IoOU2Jy3bZQ8vl3HPgEi5tSkNoyg3LveAQIL11iUnCnqYNRLLjLLaTYBxiybFk2Za1WBoto+/9Y2ak0Wgka2KNZyS9X+fozPM885tnvhOE9dFPv8XcXQAAAABmJ1DsAgAAAID5hAANAAAA5IEADQAAAOSBAA0AAADkgQANAAAA5CFU7ALytXz5cl+7dm2xywAAAMACt3PnzqPuXpd9fd4F6LVr16q1tbXYZQAAAGCBM7Pncl1nCAcAAACQBwI0AAAAkAcCNAAAAJAHAjQAAACQBwI0AAAAkAcCNAAAAJAHAjQAAACQh3m3DjQAAEXlLvlY1tdsr2V8ySULSoGgFAhNHFsgeR4Ipq6FpAD9XTjDxsaksVHJE8nHsdGJa5OuJ1JfoxPnnnWefv6F3stdev0ni/1fZBICNAAgyT35QysxLI0OJR8Tw9LosJQYmnw8mnpu2uP0PYakxEjqeCT1w3C6UDld4PRZtMnzec3U7lTBt0jGw3Q6WAcmB+9AKBW+ZxnI87nfpLYz3S/rvcfbZF7P9d7B9Deh5OlHP41Hnebr5/I+L/DzpMPkrEJn1uMpA2xWUPUcwVVegG/iF8iCBGgA85i7NDIgxXuSX4MnJo7jqeOhPsks+Q9e5g9fC6R+cAYm/2C3YLL9pHazeU1g4gfxnLwmOFF35mvGj21u/zsmRrLC5amCaD7H6XsMTbzPjPcemWg/lz80LSAFo1IwIoUiycfx/57TfdnEsSx3m0BQsvDs7jHtczO1mc19Mp6frs5T3UOaCC7jASYxOcx4IqsnMH19LCsopX4xmRKgxnLfb3R47u6H02Cpf1tyPKZ/wZj0y0fWtexfeNLPB8NSuGyWr8m8Hsi4T2jil6pZv3/2L0ZzcK/0L10lhgANLDaJkazwe2Ii/E4XijOvjY3MfP9QLKMHL1HcHrs5ZXmG7tR1H8vdkzuXAqFkOA1GpFA06zicDLGhqBSuTh1HsoJtql0omnV8ivtNOo5k3S8iBfkRsyhkB/wpvwyMTnM9M7yPpm6WK1BOdz2fR53m6+f4PnP5CzmKgn/dgPlmbEwa6p0acqcLv9lheWRg5vsHwlJZrRSrkWK1ya/as7Ku1SS/stvFqpOhKlfN6TA9PqYtfewZP3iz243N4WvGso4Tk68X6jVmU8PtjEE0nH/IHf/zN1AEgYAUiBS7CuCMIkADZ5q7NDI4i/B7YppA3KuZ/8xuySCbGXSXvygr6GaG36xQHC6b+96RQEAs+gMAWCgI0Ivd+JjWVI/mpJ7N9Hn2c73S6ODEn6unjE+cbixg+s/a2WMZs8cLTjNGMpDj9VPaWo57TvPe+bx/zvfOuO/wyZmHPWQH5VMNgwhXTO7lrV4l1Z+Xo+c3R49wpKokx4sBALBQEKDnu8RIKuCemBxwM4+zg3F2u/GxZ9MIhJLhLFqd6tlMhbXpZq8nRqZeH/+ze+b17Nn4PvXP4DN9pduV0kzhtEAoGWwzhzjUnjVD+M1oG61O/skeAACUJAJ0MY2NScP9M/T0nqIXeKj31ONZpWSPZKxmIvxWrpCWn5MRijOei2YeVxfuT/pzyT0rnOcI4GO5Qnhianh/IQHex6RIxeRQXOr/zQAAwAtW0ABtZldKulVSUNLX3f0LWc83SfqWpNpUm0+4+45C1jSnRuJZPb2z6QVOH89mLKuSk4WyA231qqzwmysIp3qLo9ULf4LR+Ixmhi0AAIDCK1iANrOgpNskXS6pXVKLmW13970ZzW6WdLe7325m50naIWltoWp6wTqekB64eWov8CmXokpN5orWTITb2jVS9KUT5zl7gDOeC8fOyEcEAADA7BSyB3qTpP3ufkCSzOwuSVdLygzQLqk6dVwj6fkC1nMaUpPEypZIS9ZO7RGergc4UslkLgAAgAWmkAF6taS2jPN2Sa/MavNZSQ+Y2VZJFZIuy3UjM7tB0g2S1NTUNOeFntKKl0l/8dMz/74AAAAoOcXuHt0i6Zvu3ijpjZL+j5lNqcnd73D3ZndvrqurO+NFAgAAAGmFDNCHJK3JOG9MXcv0Xkl3S5K7PywpJml5AWsCAAAATkshA3SLpPVmts7MIpI2S9qe1eagpEslycxeomSA7ipgTQAAAMBpKViAdvdRSTdJul/SPiVX29hjZp8zs6tSzT4i6X1m9pikbZKuc/cS3BUDAAAASCroOtCpNZ13ZF37m4zjvZJeU8gaAAAAgLlU7EmEAAAAwLxCgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8kCABgAAAPJAgAYAAADyQIAGAAAA8lDQAG1mV5rZ02a238w+keP5vzOz3amvZ8zsRCHrAQAAAE5XqFA3NrOgpNskXS6pXVKLmW13973pNu7+oYz2WyVtKFQ9AAAAwFwoZA/0Jkn73f2Auw9LukvS1TO03yJpWwHrAQAAAE5bIQP0akltGeftqWtTmNlZktZJ+vk0z99gZq1m1trV1TXnhQIAAACzVSqTCDdL+oG7J3I96e53uHuzuzfX1dWd4dIAAACACYUM0Ickrck4b0xdy2WzGL4BAACAeaCQAbpF0nozW2dmESVD8vbsRmb2YklLJD1cwFoAAACAOVGwAO3uo5JuknS/pH2S7nb3PWb2OTO7KqPpZkl3ubsXqhYAAABgrhRsGTtJcvcdknZkXfubrPPPFrIGAAAAYC6VyiRCAAAAYF4gQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5KGiANrMrzexpM9tvZp+Yps07zWyvme0xs+8Wsh4AAADgdIUKdWMzC0q6TdLlktoltZjZdnffm9FmvaRPSnqNu3ebWX2h6gEAAADmQiF7oDdJ2u/uB9x9WNJdkq7OavM+Sbe5e7ckufuRAtYDAAAAnLZCBujVktoyzttT1zKdI+kcM/sPM/u1mV1ZwHoAAACA01awIRx5vP96SX8kqVHSg2Z2vrufyGxkZjdIukGSmpqaznSNAAAAwLhC9kAfkrQm47wxdS1Tu6Tt7j7i7s9KekbJQD2Ju9/h7s3u3lxXV1ewggEAAIBTKWSAbpG03szWmVlE0mZJ27Pa/D8le59lZsuVHNJxoIA1AQAAAKelYAHa3Ucl3STpfkn7JN3t7nvM7HNmdlWq2f2SjpnZXkm/kPRRdz9WqJoAAACA02XuXuwa8tLc3Oytra3FLgMAAAALnJntdPfm7OvsRAgAAADkgQANAAAA5IEADQAAAOSBAA0AAADkgQANAAAA5IEADQAAAORh1gHazMoLWQgAAAAwH5wyQJvZq1MbnTyVOr/QzL5a8MoAAACAEjSbHui/k3SFpGOS5O6PSbq4kEUBAAAApWpWQzjcvS3rUqIAtQAAAAAlLzSLNm1m9mpJbmZhSR+QtK+wZQEAAAClaTY90H8p6f2SVks6JOnlqXMAAABg0TllD7S7H5X0Z2egFgAAAKDknTJAm9k3JHn2dXe/viAVAQAAACVsNmOgf5RxHJP0VknPF6YcAAAAoLTNZgjHDzPPzWybpIcKVhEAAABQwl7IVt7rJdXPdSEAAADAfDCbMdB9So6BttRjh6SPF7guAAAAoCTNZghH1ZkoBAAAAJgPpg3QZnbRTC90911zXw4AAABQ2mbqgf5fMzznki6Z41oAAACAkjdtgHb315/JQgAAAID5YDbrQMvMXibpPCXXgZYkufu3C1UUAAAAUKpmswrH30r6IyUD9A5Jf6zkOtAEaAAAACw6s1kH+u2SLpXU4e7vkXShpJrZ3NzMrjSzp81sv5l9Isfz15lZl5ntTn39RV7VAwAAAGfYbIZwxN19zMxGzaxa0hFJa071IjMLSrpN0uWS2iW1mNl2d9+b1fR77n5TvoUDAAAAxTBtD7SZ3WZm/0XSI2ZWK+mfJe2UtEvSw7O49yZJ+939gLsPS7pL0tVzUDMAAABQNDP1QD8j6RZJqySdlLRNyd7kand/fBb3Xi2pLeO8XdIrc7R7m5ldnHq/D7l7W3YDM7tB0g2S1NTUNIu3BgAAAApj2h5od7/V3V8l6WJJxyTdKek+SW81s/Vz9P7/Jmmtu18g6SeSvjVNLXe4e7O7N9fV1c3RWwMAAAD5O+UkQnd/zt3/p7tvkLRF0lskPTWLex/S5LHSjalrmfc+5u5DqdOvS3rFrKoGAAAAiuSUAdrMQmb2J2b2HUk/lvS0pD+dxb1bJK03s3VmFpG0WdL2rHuvzDi9StK+WVcOAAAAFMG0Y6DN7HIle5zfKOkRJScB3uDuJ2dzY3cfNbObJN0vKSjpTnffY2afk9Tq7tsl/bWZXSVpVNJxSdedzocplJNDo/rHX+zXO17RqLPrKotdDgAAAIrI3D33E2Y/l/RdST909+4zWtUMmpubvbW19Yy+57//tkvXfaNFiTHXK9ct1ZZNTbryZSsUCwfPaB0AAAA4c8xsp7s3T7k+XYAuVcUI0JJ0pDeu7+9s1/da2nTw+IBqysJ664bV2rKpSeeuqDrj9QAAAKCwCNBzZGzM9fCBY9r2yEE9sKdTw4kxbWiq1ZaNTXrzhStVHpnN3jQAAAAodQToAjh+clj37GrXXS1t2n+kX5XRkP7kwlXasmmNzl9dIzMrdokAAAB4gQjQBeTu2vlct7Y90qZ7n3he8ZExnbeyWls2rdHVG1arOhYudokAAADIEwH6DOkZHNH23Ye07ZE27T3cq1g4oDedn+yVfsVZS+iVBgAAmCcI0GeYu+uJQz3a9kibtu8+pJPDCb2ovlKbN67Rn17UqKUVkWKXCAAAgBkQoIvo5NCo7n38sLa1HNSjB08oEgzoDS9t0JZNTXrV2csUCNArDQAAUGoI0CXiqY5e3fVIm/7l0UPqGRzRWcvK9c7mNXrHKxpVXx0rdnkAAABIIUCXmPhIQvc92aFtjxzUb549rmDAdOmL67VlU5MuPqdOQXqlAQAAiooAXcIOdPXre61t+uHOdh3tH9bKmpje0bxG12xco9W1ZcUuDwAAYFEiQM8Dw6Nj+tm+Tm1radO//7ZLknTx+jpt2bRGl76kQeFgoMgVAgAALB4E6Hmm7fiAvt/aprtb29XRG9fyyqje/opGbd64RmuXVxS7PAAAgAWPAD1PjSbG9KtnurTtkTb94ukjSoy5XnX2Mm3etEZXvHSFYuFgsUsEAABYkAjQC0Bnb1w/2Nmuu1oOqu34oGrLw3rrhtXasqlJ5zRUFbs8AACABYUAvYCMjbn+83fHtK3loB7Y06GRhOuiplpt3tSkN1+wUuWRULFLBAAAmPcI0AvUsf4h3bPrkLa1HNSBrpOqioZ01ctXafPGJp3fWFPs8gAAAOYtAvQC5+5q+X237mo5qHsfP6yh0TG9dFW1Nm9q0tUvX6XqWLjYJQIAAMwrBOhFpGdwRP+6+5C2PdKmfYd7VRYO6k0XrNSWTWt0UdMSmbFJCwAAwKkQoBchd9fj7T26q+Wgtu9+XieHE1pfX6nNm5r0pxtWa0lFpNglAgAAlCwC9CJ3cmhU//bY89rW0qbH2k4oEgzoypet0OaNa/SHZy9TgK3DAQAAJiFAY9y+w736Xkub7tnVrt74qM5aVq5rNq7R21/RqPqqWLHLAwAAKAkEaEwRH0nox08e1rZH2vTIs8cVCpgufUm9Nm9q0sXr6xSkVxoAACxiBGjM6Hdd/fpeS5t+uLNdx04Oa3Vtmd7R3Kh3NK/R6tqyYpcHAABwxhUlQJvZlZJulRSU9HV3/8I07d4m6QeSNrr7jOmYAF1Yw6Nj+sneTt3VclAP7T8qSXrdOXXavLFJl76kXuFgoMgVAgAAnBlnPECbWVDSM5Iul9QuqUXSFnffm9WuStK9kiKSbiJAl4624wO6u7VNd7e2qbN3SHVVUb39FY3avHGNzlpWUezyAAAACqoYAfpVkj7r7lekzj8pSe7++ax2fy/pJ5I+Kum/EaBLz2hiTL98ukt3tRzUz586ojGXzl9do7OWlWvN0nI1LinTmiXJ41W1MUVDwWKXDAAAcNqmC9ChAr7nakltGeftkl6ZVdRFkta4+71m9tEC1oLTEAoGdNl5DbrsvAZ19MT1/dY2/frZY3riUI/ue7JDo2MTv4SZSQ1VMa1ZWqbGJeVas6RMjRkhe2VNTCGGgQAAgHmskAF6RmYWkPRlSdfNou0Nkm6QpKampsIWhhmtqIlp66XrtVXrJUmJMVdnb1xtxwfU1j2o9u4BtR0fVFv3gB559rj+dfegMvK1ggHTypqY1ixJheql5Rlhu1z1VVHWpAYAACWtkAH6kKQ1GeeNqWtpVZJeJumXqa2lV0jabmZXZQ/jcPc7JN0hJYdwFLBm5CkYMK2qLdOq2rLJf15IGR4dU0dPXG3dA2o7PqD27sHx418906UjfUOT2keCAa1eUqbGJalQvbRsUtheVhFhK3IAAFBUhQzQLZLWm9k6JYPzZknvSj/p7j2SlqfPzeyXmsUYaMwvkVBATcvK1bSsPOfz8ZGE2tM9192Das8I2Xue79Dxk8OT2peFgxM91xkhO92DXVMePhMfCwAALGIFC9DuPmpmN0m6X8ll7O509z1m9jlJre6+vVDvjfkjFg7qRfWVelF9Zc7n+4dG1d49oPbj6Z7ribDd8uxx9Q2NTmpfFQuNj73OnuDYuKRMFdGijVoCAAALBBupYN5yd/UOjqqte2DS2Ov27sHUmOwBxUfGJr1maUVkPFQ3Zkx0XLO0XKtryxQLs4IIAABIKsYqHEBBmZlqysOqKa/Ry1bXTHne3XXs5PCUCY7t3QPae7hXP9nbqeHE5IBdXxXN6rmeGB6ysjbGRjIAAIAAjYXLzLS8MqrllVFtaFoy5fmxMdeRvqHJExxTPdc7n+vWjx4/rETGEiIBk1bW5J7guLKmTPXVUXqwAQBYBAjQWLQCAdOKmphW1MS0ce3SKc+PJCZWEGnPGHvddnxA/7H/qDr74soeAVUdC6m+OqaG6qjqq2Kqr4qqvjr52FCdPo+qPML/9QAAmK/4KQ5MIxwMpNapLpf+YOrzQ6MJHeoeVFv3oDp74+rqG1Jnb1xHeofU2RfXI88eV1ff0JRhIpJUFQ2prjo6OVhXxVSfCt4N1cngXcmkRwAASg4/nYEXKBoK6uy6Sp1dl3sFESk5DvvEwIiO9A3pSF9cnb3JxyMZj48ePKHO3riGRqcG7fJIUA3VMdVVRaf0YjekAnddVUzVsRDrYwMAcIYQoIECMjMtqYhoSUVE566omradu6s3PqojvfHJYTsjaD95qEc/23dEgyOJKa+PhQMTPddVycA9KWynjmvKwgRtAABOEwEaKAFmppqysGrKwlrfMHPQ7h8aTYbsrN7sdO/2vsO9+tUzQ+rPWiNbSm5sU5/qzc4cKpI9VntJOUEbAIDpEKCBecTMVBULqyoW1h/MMHREkk6OB+1kr3Z6nHb6eH9Xv/7zd0fVG58atMNBU11lZrieGDJSnzFme1lFRIEAQRsAsLgQoIEFqiIa0rpoSOuWV8zYLj6SmNKLnRm4f3/spB75/XGdGBiZ8tpQILlUYHryY331RO92XVVUdVVRLa+MaHklS/wBABYOAjSwyMXCQTUtK1fTsvIZ28VHEuM92F1ZEyI7+4bU3j2gRw9269jJ4Zyvr46FxkN1XVVMdZXRjPPo+PnSioiC9GoDAEoYARrArMTCwYll/WYwkhjTsf5hdfUNqas/2Ys9/tWffHyi/YS6+oZ0cnjqhMiAScsqk4F6eUawzg7adVVRVh8BABQFARrAnAoHA+Mb1EhTt1jPdHJoVEf7h3S0fyhn0O7qG9L+zj519Q9pJOFTXh8JBWYM2Msrk0NK6qoYQgIAmDsEaABFUxENqSIa0lnLZh6n7e7qGRzJGbDTx23HJ4aQZO8QKaU2r6lK9WpP07NdnxpCEgoGCvSJAQALAQEaQMkzM9WWR1RbHplxmT9JGk2M6fjJ4eRY7Wl6tvc936sH+4bUl2OpPzNpWUVy4uN0QTt9zLraALA4EaABLCihYCC51F517JRt0xMjpwvaXX1DOtB1Ul39QxrOsVNkerm/XAF7ecb1+qqYyiIMIQGAhYIADWDRmu3EyPROkdkBO3Ps9vMn4nqsvUfH+oc0Nt0Qkoxl/tLra2cesy07AMwPBGgAOIXMnSJfVD/zBjaJMdfxk8OpJf8mNq9Jnx/pHdLuthM60hdXfGRqr3Y0FJgcrFO7RNZl7CBZXx3V0nI2sQGAYiFAA8AcCgZsfOjGeaqetp27q29odHwTm67M7dlTx8909umh/UfVl2O3yMmb2CR7r8d7sjN2kayriirMpEgAmFMEaAAoAjNTdSys6tipe7Uzd4vM3J49/dXePahHD56YdhObpRWR8TCdvWNk5jHjtAFgdgjQAFDiZrtb5EhiTEf70z3ZE0NG0rtHHukb0v4j/erqG9JojoHajNMGgNkhQAPAAhEOBrSypkwra8pmbDc25uoeGJ7oxU71aDNOGwBmhwANAItMIGBaVhnVssqoXrJy+nanO047GDAtr4xk9GAnHxuqY2qoTj7WV0e1rCKqIEEbwDxCgAYA5JTPOO3B4cRED3aOcdqHTgxqd1vucdrpoN1QHVN91US4bqieHLrp0QZQKhZEgB4ZGVF7e7vi8XixSzmjYrGYGhsbFQ6Hi10KgEWuLDK7cdrDo8lx2p29cXWmerM7e5PDRjr7htTePaCdzx1X98DIlNeGAjbek92QGkKSDtnjvdpVMdWWs0MkgMIqaIA2sysl3SopKOnr7v6FrOf/UtL7JSUk9Uu6wd335vs+7e3tqqqq0tq1axfNP5rurmPHjqm9vV3r1q0rdjkAMCuRUECrasu0qnbmcdrpXSLTvdmdvXF19g2Nh+1nj57Urw8cV8/g1KAdCQZUVxXN6MlODhVpSI3NbqiOqaEqpuoyJkMCeGEKFqDNLCjpNkmXS2qX1GJm27MC8nfd/Wup9ldJ+rKkK/N9r3g8vqjCs5T80+qyZcvU1dVV7FIAYM7NdpfI9BJ/nZN6slOPvfHkGO3fHlXf0NQx2tFQYGKoSGa4Hg/byWtVUYI2gMkK2QO9SdJ+dz8gSWZ2l6SrJY0HaHfvzWhfISnHBrizsxj/cVuMnxkAMs12ib+B4dHxUN2Z0at9JNWrve9wr37xdFwDw4kpry0LBycPFamamACZOWa7IrogRkUCmIVC/r99taS2jPN2Sa/MbmRm75f0YUkRSZfkupGZ3SDpBklqamqa80IBAAtbeSSktctDWru8YsZ2/UOj4z3Z6fHZnangfaRvSE+0n9BPenMv71cZDY1vTDM+dCTHMRvWAPNf0X9ddvfbJN1mZu+SdLOkd+doc4ekOySpubn5BfdSF1JlZaX6+/uLXQYA4DRURkOqrKvUH9RNv+rIxPJ+mRMhJ8Znd/bGtetgtzp7hzQ8OjVoV8VC40NFVlSXaWVNTCtqYlpRnXxcWRPT0ooIf2UESlghA/QhSWsyzhtT16Zzl6TbC1gPAACnbfLyflXTtnN39Q6Ojo/PTofsrtSwkY7euB7+3VF19g0pkbUzZCQUSAbqjFA98VimFamNa1g/GyiOQgboFknrzWydksF5s6R3ZTYws/Xu/tvU6Zsk/Van6b//2x7tfb731A3zcN6qav3tn7x0Vm3dXR/72Mf04x//WGamm2++Wddcc40OHz6sa665Rr29vRodHdXtt9+uV7/61Xrve9+r1tZWmZmuv/56fehDH5rT2gEAxWFmqikPq6Y8rHMapg/aiTHX0f4hHe6Jq6NnUB09cR3ujScfe+La3XZC9z0Z13Bicm92MLWs33iwri7TipqoVtSkerVTQ0cioUChPyqw6BQsQLv7qJndJOl+JZexu9Pd95jZ5yS1uvt2STeZ2WWSRiR1K8fwjfnmnnvu0e7du/XYY4/p6NGj2rhxoy6++GJ997vf1RVXXKFPf/rTSiQSGhgY0O7du3Xo0CE9+eSTkqQTJ04UuXoAwJkWDNj4OGmtqc3Zxt3VPTCiw+mA3ZMM2B2poP10R59++XRXzkmQyyujyWCdMVxkZcaQkRU1MZVHij6iE5hXCvr/GHffIWlH1rW/yTj+wFy/52x7igvloYce0pYtWxQMBtXQ0KDXve51amlp0caNG3X99ddrZGREb3nLW/Tyl79cZ599tg4cOKCtW7fqTW96k97whjcUtXYAQGkyMy2tiGhpRUQvXVWTs016bHZnRsA+3BNXR28ydLd3D6j1ueM6kWOTmpqycM6x2OO92TUxlvMDMvAr5xly8cUX68EHH9S9996r6667Th/+8If153/+53rsscd0//3362tf+5ruvvtu3XnnncUuFQAwD2WOzV4/w5CRweGEOnrjOtwzqM7erLDdE9ee53t1tH9InjVlvzwSnDRcZGVNTA01Ma1k8iMWIQL0HHvta1+rf/qnf9K73/1uHT9+XA8++KBuueUWPffcc2psbNT73vc+DQ0NadeuXXrjG9+oSCSit73tbTr33HN17bXXFrt8AMACVxYJat3yCq2bYUm/4dExHembCNaTg/bg9JMfg4EcvdjJx4bqmFbWlDH5EQsCAXqOvfWtb9XDDz+sCy+8UGamL37xi1qxYoW+9a1v6ZZbblE4HFZlZfBmA30AAAyRSURBVKW+/e1v69ChQ3rPe96jsbHkxJDPf/7zRa4eAIDkKiCNS8rVuGT6DWoSY65jqcmP6QmQh3vj40NIHms/ofv2xKcs5Zc9+TEZrGNMfsS8Yp79N5oS19zc7K2trZOu7du3Ty95yUuKVFFxLebPDgAobenJjx2psdiZw0XSvdqHTwzqZM7Jj5FUb/bktbLHj5n8iDPAzHa6e3P2db7zAABAQWROfjxvVfW07XrjI+pMrSoyeUz2oNq7B7TzuePqzjH5sToW0sqasknDRSaGjySvV8eY/Ii5R4AGAABFNZvJj/GRRNbKIkPJYSOp4L338AyTHzOW7MvcjCYdupeWRxRgXDbyQIAGAAAlLxYOau3yCq2dYfLjSGJMR/oygvV44E4e/+bAcXX2xjWaY/JjfXV0yljszNBdVxlVKMi4bCQRoAEAwIIQDga0urZMq2vLpm2TnvyYa7hIR29cT7Sf0AN74hrKmvwYMKm+aurSfRNjs8vUUBNVNBQs9MdECSBAAwCARSMYMNVXx1RfHdMFjbnbuLtODIxkLeE3MVxkf1e//mP/UfUNjU557bKKyLQb0qRXHKmIEr/mO/4XBAAAyGBmWlIR0ZJTTH7si49MrCbSk1rCLzVc5PmeuHYd7M45+bEqFsoYix2dtONjevhITVmYyY8ljAANAADwAlTFwqqKhfWi+pknP2bv+NiZ2gmyoyeupw73qivH5MdYODC+JvaKzBVGMs4Zl108BOgSNDo6qlCI/2kAAJjvYuGgzlpWobOWzWby48Rujx2p4SKdvXHtfK5bR3qHNJyYOi67rio6HrTT26uPh+xq1ssulIX3X/THn5A6npjbe644X/rjL8zY5OTJk3rnO9+p9vZ2JRIJfeYzn9HZZ5+tD3zgAzp58qSi0ah+9rOfKRwO68Ybb1Rra6tCoZC+/OUv6/Wvf72++c1v6p577lF/f78SiYR27NihrVu36sknn9TIyIg++9nP6uqrr57bzwUAAIpuNpMf3V3HTw6P92B3ZOz62NEb1++PndTDB46pLz51XHZ1LKQVmbs+Vk8N2ksrIgwZycPCC9BFct9992nVqlW69957JUk9PT3asGGDvve972njxo3q7e1VWVmZbr31VpmZnnjiCT311FN6wxveoGeeeUaStGvXLj3++ONaunSpPvWpT+mSSy7RnXfeqRMnTmjTpk267LLLVFEx/W+wAABgYTIzLauMalllVC9bXTNtu4Hh0fGe7I7eiSX8OlLB++mOvpxDRiLBgBpqsnqzMyZCNlTHVF/FFutpCy9An6KnuFDOP/98feQjH9HHP/5xvfnNb1Ztba1WrlypjRs3SpKqq5OTEB566CFt3bpVkvTiF79YZ5111niAvvzyy7V06VJJ0gMPPKDt27frS1/6kiQpHo/r4MGDbNsNAACmVR4J6ey6Sp1dVzltm9HEmLr6h8YnPmYH7ScP9egnezunLOUnScsro1qRCtrp3uuG6vTOj1E1VMdUFQsX8iOWhIUXoIvknHPO0a5du7Rjxw7dfPPNuuSSS/K+R2bvsrvrhz/8oc4999y5LBMAACxyoWBAK2vKtLJm5iEjPYMj4+tld2ZsSNPRG1d796Ban+vWiRyrjFREguMTHScNG8lYL3t5RXRe7/5IgJ4jzz//vJYuXaprr71WtbW1+upXv6rDhw+rpaVFGzduVF9fn8rKyvTa175W3/nOd3TJJZfomWee0cGDB3Xuuedq165dk+53xRVX6Ctf+Yq+8pWvyMz06KOPasOGDUX6dAAAYDExM9WWR1RbHtGLV0y/lF96lZGOrICdfvz1746ps29IiazdH0MBU0N1TA3V0VRP9kQP9srUVuv11VHFwqW5MQ0Beo488cQT+uhHP6pAIKBwOKzbb79d7q6tW7dqcHBQZWVl+ulPf6q/+qu/0o033qjzzz9foVBI3/zmNxWNRqfc7zOf+Yw++MEP6oILLtDY2JjWrVunH/3oR0X4ZAAAALnNZpWR7N0fOzOGi3T0xvVUR59++XSXBoYTU167pDysFTVl2n7TaxQuoSX7zLNHkZe45uZmb21tnXRt3759i3Zs8GL+7AAAYGFwd/UNjU5aWSQ9bKRncET/+K6LilKXme109+bs6/RAAwAAoKjMTNWxsKpjYa1vmH5jmlJROn3hAAAAwDywYAL0fBuKMhcW42cGAAAotgURoGOxmI4dO7aoAqW769ixY4rFYsUuBQAAYFEp6BhoM7tS0q2SgpK+7u5fyHr+w5L+QtKopC5J17v7c/m+T2Njo9rb29XV1TUHVc8fsVhMjY2NxS4DAABgUSlYgDazoKTbJF0uqV1Si5ltd/e9Gc0eldTs7gNmdqOkL0q6Jt/3CofDWrdu3VyUDQAAAMyokEM4Nkna7+4H3H1Y0l2Srs5s4O6/cPeB1OmvJdGdCgAAgJJWyAC9WlJbxnl76tp03ivpxwWsBwAAADhtJbEOtJldK6lZ0uumef4GSTdIUlNT0xmsDAAAAJiskAH6kKQ1GeeNqWuTmNllkj4t6XXuPpTrRu5+h6Q7Uu27zCzviYZzZLmko0V6b5Q2vjcwHb43MB2+NzATvj9Kw1m5LhZsK28zC0l6RtKlSgbnFknvcvc9GW02SPqBpCvd/bcFKWQOmVlrru0cAb43MB2+NzAdvjcwE74/SlvBxkC7+6ikmyTdL2mfpLvdfY+Zfc7Mrko1u0VSpaTvm9luM9teqHoAAACAuVDQMdDuvkPSjqxrf5NxfFkh3x8AAACYawtiJ8Iz6I5iF4CSxfcGpsP3BqbD9wZmwvdHCSvYGGgAAABgIaIHGgAAAMgDARoAAADIAwF6FszsSjN72sz2m9knil0PSoOZrTGzX5jZXjPbY2YfKHZNKC1mFjSzR83sR8WuBaXFzGrN7Adm9pSZ7TOzVxW7JpQGM/tQ6mfKk2a2zcxixa4JUxGgT8HMgpJuk/THks6TtMXMzituVSgRo5I+4u7nSfpDSe/newNZPqDkMp5Atlsl3efuL5Z0ofg+gSQzWy3pryU1u/vLJAUlbS5uVciFAH1qmyTtd/cD7j4s6S5JVxe5JpQAdz/s7rtSx31K/gBcXdyqUCrMrFHSmyR9vdi1oLSYWY2kiyX9b0ly92F3P1HcqlBCQpLKUhvSlUt6vsj1IAcC9KmtltSWcd4uQhKymNlaSRsk/aa4laCE/L2kj0kaK3YhKDnrJHVJ+kZqiM/Xzayi2EWh+Nz9kKQvSToo6bCkHnd/oLhVIRcCNHCazKxS0g8lfdDde4tdD4rPzN4s6Yi77yx2LShJIUkXSbrd3TdIOimJ+TWQmS1R8q/c6yStklRhZtcWtyrkQoA+tUOS1mScN6auATKzsJLh+Tvufk+x60HJeI2kq8zs90oO+7rEzP5vcUtCCWmX1O7u6b9Y/UDJQA1cJulZd+9y9xFJ90h6dZFrQg4E6FNrkbTezNaZWUTJwfzbi1wTSoCZmZJjGPe5+5eLXQ9Kh7t/0t0b3X2tkv9m/Nzd6UWCJMndOyS1mdm5qUuXStpbxJJQOg5K+kMzK0/9jLlUTDAtSaFiF1Dq3H3UzG6SdL+Ss2HvdPc9RS4LpeE1kv6rpCfMbHfq2qfcfUcRawIwP2yV9J1Ux8wBSe8pcj0oAe7+GzP7gaRdSq709KjY0rsksZU3AAAAkAeGcAAAAAB5IEADAAAAeSBAAwAAAHkgQAMAAAB5IEADAAAAeSBAA8A8YmYJM9ud8TVnO9iZ2Voze3Ku7gcACxXrQAPA/DLo7i8vdhEAsJjRAw0AC4CZ/d7MvmhmT5jZI2b2otT1tWb2czN73Mx+ZmZNqesNZvYvZvZY6iu9XXDQzP7ZzPaY2QNmVla0DwUAJYoADQDzS1nWEI5rMp7rcffzJf2jpL9PXfuKpG+5+wWSviPpH1LX/0HSr9z9QkkXSUrvsLpe0m3u/lJJJyS9rcCfBwDmHXYiBIB5xMz63b0yx/XfS7rE3Q+YWVhSh7svM7Ojkla6+0jq+mF3X25mXZIa3X0o4x5rJf3E3denzj8uKezu/6PwnwwA5g96oAFg4fBpjvMxlHGcEHNlAGAKAjQALBzXZDw+nDr+T0mbU8d/JunfU8c/k3SjJJlZ0MxqzlSRADDf0bMAAPNLmZntzji/z93TS9ktMbPHlexF3pK6tlXSN8zso5K6JL0ndf0Dku4ws/cq2dN8o6TDBa8eABYAxkADwAKQGgPd7O5Hi10LACx0DOEAAAAA8kAPNAAAAJAHeqABAACAPBCgAQAAgDwQoAEAAIA8EKABAACAPBCgAQAAgDz8f36jzixhyLr/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}